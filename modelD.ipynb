{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_convert(tensor):\n",
    "    \"\"\" Display a tensor as an image. \"\"\"\n",
    "    \n",
    "   # image = tensor.to(\"cpu\").clone().detach()\n",
    "    image = image.numpy().squeeze()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
    "    image = image.clip(0, 1)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformation to be applied to each image\n",
    "# x=(x-mean)/std\n",
    "#TODO why\n",
    "\"\"\"Most of the pretrained models require the input to be 224x224 images. Also,\n",
    "we'll need to match the normalization used when the models were trained.\n",
    "Each color channel was normalized separately, the means are [0.485, 0.456, 0.406] \n",
    "and the standard deviations are [0.229, 0.224, 0.225].\"\"\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    # transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "    # transforms.Lambda(lambda x: x * 255.0)\n",
    "    transforms.CenterCrop((224,224)),\n",
    "    transforms.RandomHorizontalFlip(), # randomly flip and rotate\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "])\n",
    "\n",
    "BATCH_SIZE=5\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 2\n",
    "image_datasets = {x: datasets.ImageFolder('./ImageByClasses', transform=transform)\n",
    "                  for x in ['train', 'valid']}\n",
    "                  \n",
    "loaders_scratch = {\n",
    "    x: torch.utils.data.DataLoader(image_datasets[x], shuffle=True, batch_size=BATCH_SIZE, num_workers=num_workers)\n",
    "    for x in ['train', 'valid']}\n",
    "\n",
    "# # Load the images from the folder\n",
    "# dataset = datasets.ImageFolder('./ImageByClasses', transform=transform)\n",
    "\n",
    "# BATCH_SIZE=5\n",
    "# # Create a data loader to load the images in batches\n",
    "# data_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# # Iterate over the data loader to get the tensors\n",
    "# for images, labels in data_loader:\n",
    "#     # Do something with the tensors\n",
    "#     print(images)\n",
    "#     # for image in images:\n",
    "#     #     # print(image_convert(image))\n",
    "#     #     plt.imshow(image_convert(image))\n",
    "        \n",
    "    \n",
    "#     # # Convert the tensor to NumPy array\n",
    "#     # images_np = images.numpy()\n",
    "\n",
    "#     # # Iterate through each image in the batch\n",
    "#     for i in range(images_np.shape[0]):\n",
    "#         # Extract the current image from the batch\n",
    "#         current_image = images_np[i, :, :, :]\n",
    "\n",
    "#         # Transpose the dimensions to match the order expected by matplotlib (H x W x C)\n",
    "#         current_image = current_image.transpose(1, 2, 0)\n",
    "\n",
    "#         # Display the image using matplotlib\n",
    "#         plt.imshow(image_convert(images[i]))\n",
    "#         plt.title(f\"Image {i + 1} : {labels[i]}\")\n",
    "#         plt.show()\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# class Dog(nn.Module):\n",
    "#     def __init__(self,num_classes=120):\n",
    "#         super(Dog, self).__init__()\n",
    "#         self.vgg16 = models.vgg16(pretrained=True)  # Cargar el modelo VGG-16 preentrenado\n",
    "#         # Congelar las capas para que los pesos no se actualicen durante el entrenamiento\n",
    "#         for param in self.vgg16.parameters():\n",
    "#             param.requires_grad = False\n",
    "#         # Modificar la última capa para adaptarse al número de clases\n",
    "#         in_features = self.vgg16.classifier[6].in_features\n",
    "#         self.vgg16.classifier[6] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.vgg16(x)\n",
    "num_classes = 120\n",
    "model = models.vgg16(pretrained=True)  \n",
    "\n",
    "# Cargar el modelo VGG-16 preentrenado\n",
    "# Congelar las capas para que los pesos no se actualicen durante el entrenamiento\n",
    "\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modificar la última capa para adaptarse al número de clases\n",
    "in_features = model.classifier[6].in_features\n",
    "model.classifier[6] = nn.Linear(in_features, num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=120, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 20\n",
    "batch_size = 16\n",
    "learning_rate = 0.005\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  \n",
    "\n",
    "\n",
    "# Train the model\n",
    "total_step = len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, train_dataloader):\n",
    "    #model.train() #placer le modèle en mode apprentissage\n",
    "        \n",
    "    for X,y in train_dataloader:#demarrage de l'apprentissage\n",
    "        # X,y = X.to(device), y.to(device)\n",
    "        preds = model(X)\n",
    "        preds = preds[:, 0]\n",
    "        erreur = objectif(preds, y )\n",
    "        erreur.backward()\n",
    "        sgd.step()            \n",
    "        sgd.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, val_dataloader):\n",
    "#     model.eval() #placer le modèle en mode prediction  \n",
    "    all_preds, all_targets = [], []\n",
    "    for X,y in val_dataloader:\n",
    "     #    X,y = X.to(device), y.to(device)\n",
    "        preds = model(X)\n",
    "        preds = preds[:, 0]\n",
    "        all_preds.append( preds )\n",
    "        all_targets.append( y )\n",
    "    all_preds   = torch.cat( all_preds )\n",
    "    all_targets = torch.cat( all_targets )\n",
    "    return all_preds, all_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold( n_splits=2, shuffle=True, random_state=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def afficher(data_loader):\n",
    "     for images, labels in data_loader:\n",
    "          for i in range(images_np.shape[0]):\n",
    "               plt.imshow(image_convert(images[i]))\n",
    "               plt.title(f\"Image {i + 1} : {labels[i]}\")\n",
    "               plt.show()\n",
    "          break\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  \n",
    "\n",
    "\n",
    "# Train the model\n",
    "total_step = len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, train_loader, valid_loader,\n",
    "          model, optimizer, criterion, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "          #   # move to GPU\n",
    "          #   if use_cuda:\n",
    "          #       data, target = data.cuda(), target.cuda()\n",
    "                \n",
    "            ## find the loss and update the model parameters accordingly\n",
    "            ## record the average training loss, using something like\n",
    "            ## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            \n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            output = model(data)\n",
    "            # calculate batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # parameter update\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "            \n",
    "        ######################\n",
    "        # validate the model #\n",
    "        ######################\n",
    "        for batch_idx, (data, target) in enumerate(valid_loader):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "            ## update the average validation loss\n",
    "            \n",
    "            # forward pass\n",
    "            output = model(data)\n",
    "            # batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update validation loss\n",
    "            valid_loss += loss.item() * data.size(0)\n",
    "        \n",
    "        # calculate average losses\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        valid_loss = valid_loss / len(valid_loader.dataset)\n",
    "        \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {}\\tTraining Loss: {:.6f}\\t Validation Loss: {:.6f}'.\n",
    "             format(epoch, train_loss, valid_loss))\n",
    "        \n",
    "        ## TODO: save the model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).    Saving model...'.\n",
    "                 format(valid_loss_min, valid_loss))\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            valid_loss_min = valid_loss\n",
    "    \n",
    "    # return trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_loaders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [43], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m n_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m train(n_epochs, \u001b[43mdata_loaders\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m], data_loaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m], model,\n\u001b[0;32m      4\u001b[0m                      optimizer_scratch, criterion_scratch, use_cuda, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_scratch.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_loaders' is not defined"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "# train the model\n",
    "model = train(n_epochs, data_loaders['train'], data_loaders['valid'], model,\n",
    "                     optimizer_scratch, criterion_scratch, 'model_scratch.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
